{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Mohsen Ghazel (mghazel)\n",
    "* Date: April 7th, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project: Object Tracking:\n",
    "* The objective of this project is to demonstrate the \n",
    "  tracking of a detected moving object using built-in \n",
    "  OpenCV Python tracking functionalities:\n",
    "     * Optical flow\n",
    "     * Dense optical flow\n",
    "     * Mean-Shift\n",
    "     * Cam-Shift\n",
    "     * BOOSTING Tracker\n",
    "     * MIL\n",
    "     * TLD\n",
    "     * MEDIAN FLOW\n",
    "       \n",
    " * We shall assume the following:\n",
    "   * The moving camera is moving\n",
    "   * The object of interest is also moving\n",
    "   * We have the bounding box location of the object of interest from \n",
    "     the first video frame.\n",
    " * Our object is to track the object of interest in the remaining frames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Imports and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV :  3.4.8\n",
      "Numpy :  1.19.2\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------\n",
    "# Python imports and environment setup\n",
    "#------------------------------------------------------\n",
    "# opencv\n",
    "import cv2\n",
    "# numpy\n",
    "import numpy as np\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# input/output OS\n",
    "import os \n",
    "\n",
    "# date-time to show date and time\n",
    "import datetime\n",
    "\n",
    "# Use %matplotlib notebook to get zoom-able & resize-able notebook. \n",
    "# - This is the best for quick tests where you need to work interactively.\n",
    "%matplotlib notebook\n",
    "\n",
    "#------------------------------------------------------\n",
    "# Test imports and display package versions\n",
    "#------------------------------------------------------\n",
    "# Testing the OpenCV version\n",
    "print(\"OpenCV : \",cv2.__version__)\n",
    "# Testing the numpy version\n",
    "print(\"Numpy : \",np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read the input video:\n",
    "* We shall assume the following:\n",
    "    * The moving camera is moving\n",
    "    * The object of interest is also moving\n",
    "    * We have the bounding box location of the object of \n",
    "      interest from the first video frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input video file: ../resources/videos/cyclist.mp4 has 3254 frames.\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------\n",
    "# Open camera video file\n",
    "#----------------------------------------------------\n",
    "# the source video file name\n",
    "video_file_path = \"../resources/videos/cyclist.mp4\"\n",
    "# check if the reference image file exists\n",
    "if(os.path.exists(video_file_path) == 0):\n",
    "    print('Video file name DOES NOT EXIST! = ' + video_file_path)\n",
    "# open the video file\n",
    "cap = cv2.VideoCapture(video_file_path)\n",
    "# check the status of the opened video file\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot read video file: \" + video_file_path)\n",
    "    exit();\n",
    "# get the number of frames in the video file\n",
    "num_video_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# display a message\n",
    "print(\"Input video file: {0} has {1} frames.\".format(video_file_path, num_video_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  Grab the first frame of the video for manual annotation of the object of interest:\n",
    "* Read the video frames \n",
    "* Save the first frame to manually annotate the object of interest:\n",
    "  * Object of interest annotation: the four corners of the bounding-box\n",
    "    * TLC = (x1, y1)\n",
    "    * TRC = (x2, y2)\n",
    "    * BRC = (x3, y3)\n",
    "    * BLC = (x4, y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab and save the first video frame.\n",
    "for counter in range(1):\n",
    "    #--------------------------------------------\n",
    "    # Step 1: read the next video frame\n",
    "    #--------------------------------------------\n",
    "    ret, first_frame = cap.read();\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    # Step 2: save the first frame for annotation\n",
    "    #--------------------------------------------\n",
    "    # the first frame vfile name\n",
    "    output_file_path = \"../results/frame-\" + str(counter) + \".jpg\"\n",
    "    # save the frame\n",
    "    cv2.imwrite(output_file_path, first_frame);\n",
    "\n",
    "#--------------------------------------------\n",
    "# clear the video capture object:\n",
    "#--------------------------------------------\n",
    "cap.release();\n",
    "# close all windows\n",
    "cv2.destroyAllWindows();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:  Manually annotate of the object of interest from the first frame:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1)  Manually annotate of the object of interest from the first frame:\n",
    "\n",
    "* The bounding-box of the object of interest annotation has the following\n",
    "  coordinates:\n",
    "  * b-box: (tlc_x, tlc_y, width, height) = (445, 100, 76, 183)\n",
    "    * TLC = (445, 100)\n",
    "    * TRC = (521, 100)\n",
    "    * BRC = (521, 283)\n",
    "    * BLC = (445, 283)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# The object of interest manually annotated \n",
    "# bounding-box:\n",
    "#--------------------------------------------\n",
    "# TLC coordinates: (tlc_x, tlc_y)\n",
    "# tlc-x\n",
    "tlc_x = 445\n",
    "# tlc-y\n",
    "tlc_y = 100\n",
    "# the bounding-box dimension\n",
    "# width\n",
    "width = 76\n",
    "# height\n",
    "height = 183"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Display the annotated bounding box of the object of interest on the first frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# Step 1: Read the first frame image\n",
    "#--------------------------------------------\n",
    "# the first frame file name\n",
    "first_frame_file_path = \"../results/frame-\" + str(counter) + \".jpg\"\n",
    "# read the frame image\n",
    "first_frame = cv2.imread(output_file_path);\n",
    "\n",
    "#----------------------------------------\n",
    "# Step 2: draw a GREEN rectangle to \n",
    "#        visualize the bounding rect\n",
    "#        of the object of interest.\n",
    "#----------------------------------------\n",
    "cv2.rectangle(first_frame, (tlc_x, tlc_y), (tlc_x + width, tlc_y + height), (0, 255, 0), 3)\n",
    "\n",
    "#----------------------------------------\n",
    "# display the frame image with the \n",
    "# overlaid object of interest \n",
    "# bounding-box\n",
    "#----------------------------------------\n",
    "cv2.imshow(\"Object of interest\", first_frame)\n",
    "# wait to visualize the image\n",
    "cv2.waitKey(0)\n",
    "# close all windows\n",
    "cv2.destroyAllWindows();\n",
    "\n",
    "# save the figure\n",
    "# the first frame file name\n",
    "first_frame_file_path = \"../results/frame-\" + str(counter) + \"-bbox-overlaid.jpg\"\n",
    "# save the frame\n",
    "cv2.imwrite(first_frame_file_path, first_frame);\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Track the object of interest:\n",
    "\n",
    "* We are now ready to start tracking the moving object of \n",
    " interest bounding-box using built-in \n",
    "  OpenCV Python tracking functionalities:\n",
    "     * Optical flow\n",
    "     * Dense optical flow\n",
    "     * Mean-Shift\n",
    "     * Cam-Shift\n",
    "     * BOOSTING Tracker\n",
    "     * MIL\n",
    "     * TLD\n",
    "     * MEDIAN FLOW\n",
    "       \n",
    "       \n",
    "### 5.1) Tracker:  Lucas Kanade Optical Flow\n",
    "\n",
    "* In this section, we shall compute Shi-Tomasi corners and track them using the Lucas-Kanade Optical Flow tracker:\n",
    "  * From the first frame, we compute compute Shi-Tomasi corners from within the orbject of interest ROI selected above\n",
    "  * Then we apply the Lucas-Kanade Optical Flow algorithm to track these features in the seusequent frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1) Set the Shi-Tomasi corner detector parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Shi-Tomasi corner detection (good features to track paper)\n",
    "corner_track_params = dict(maxCorners = 10,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2) Set the Lucas-Kanade optical flow parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (200,200),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10,0.03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3) Create a mask to focus on the selected object of interest ROI:\n",
    "* Anything outside the object of interest bounding-box will be masked-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a zero-mask with  the same size as the image\n",
    "roi_mask = np.uint8(np.zeros((img.shape[0], img.shape[1]), dtype=int))\n",
    "# set the pixels inside the object of interest bounding-box to 1\n",
    "roi_mask[tlc_y: tlc_y + height, tlc_x: tlc_x + width] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.4) Read the first frame and compute features within the object of interest ROI to track:\n",
    "* We only compute Harris corners inside the object of interest ROI \n",
    "* we shall then track these features in subsequent frames,  using the Optical flow algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of detected Harris corners = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-97-f57ed679c4e5>:29: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  cv2.circle(first_frame, (temp[0], temp[1]), 5, (0, 0, 255), 3)\n"
     ]
    }
   ],
   "source": [
    "# open the video file\n",
    "cap = cv2.VideoCapture(video_file_path)\n",
    "# check the status of the opened video file\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot read video file: \" + video_file_path)\n",
    "    exit();\n",
    "\n",
    "# Grab the very first frame of the stream\n",
    "ret, prev_frame = cap.read()\n",
    "\n",
    "# Convert to grayscale image if it is RGB\n",
    "if ( len(prev_frame.shape) > 2 ):\n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "else:\n",
    "    prev_gray = prev_frame.copy()\n",
    "\n",
    "# Since we only want to compute Harris corners features to track \n",
    "# inside the object of interest ROI:\n",
    "#  - we need to apply the mask\n",
    "prev_gray = np.uint8(np.multiply(prev_gray, roi_mask))\n",
    "\n",
    "# compute the Harris corner\n",
    "prevPts = cv2.goodFeaturesToTrack(prev_gray, mask = None, **corner_track_params)\n",
    "# display a message\n",
    "print('Number of detected Harris corners = ' + str(prevPts.shape[0]))\n",
    "# Drawing circles around corners\n",
    "for i in range(prevPts.shape[0]):\n",
    "    temp = np.squeeze(prevPts[i])\n",
    "    cv2.circle(first_frame, (temp[0], temp[1]), 5, (0, 0, 255), 3)\n",
    "# Showing the result\n",
    "cv2.namedWindow(\"Detected Harris corners\")\n",
    "# display the image with overlaid features\n",
    "cv2.imshow(\"Detected Harris corners\", first_frame)\n",
    "# save the figure\n",
    "# the first frame file name\n",
    "first_frame_file_path = \"../results/frame-1-harris-corners-overlaid.jpg\"\n",
    "# save the frame\n",
    "cv2.imwrite(first_frame_file_path, first_frame);\n",
    "# wait to visualize the image\n",
    "cv2.waitKey(0)\n",
    "# close all windows\n",
    "cv2.destroyAllWindows();  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.5) Track the computed features within the object of interest ROI to track using Lucas-Kanade Optical Flow\n",
    "* Next, we start tracking these features in subsequent frames, using the Lucas-Kanade Optical flow algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-98-5b5ecd1f3ea4>:32: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  mask = cv2.line(mask, (x_new,y_new),(x_prev,y_prev), (0,255,0), 3)\n",
      "<ipython-input-98-5b5ecd1f3ea4>:35: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  frame = cv2.circle(frame,(x_new,y_new),8,(0,0,255),-1)\n"
     ]
    }
   ],
   "source": [
    "# Create a matching mask of the previous frame for drawing on later\n",
    "mask = np.zeros_like(prev_frame)\n",
    "\n",
    "# frame counter\n",
    "frame_counter = 1;\n",
    "\n",
    "# read the subsequent video frames\n",
    "while True:\n",
    "    \n",
    "    # Grab current frame\n",
    "    ret,frame = cap.read()\n",
    "    \n",
    "    # Grab gray scale\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate the Optical Flow on the Gray Scale Frame\n",
    "    nextPts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, frame_gray, prevPts, None, **lk_params)\n",
    "    \n",
    "    # Using the returned status array (the status output)\n",
    "    # status output status vector (of unsigned chars); each element of the vector is set to 1 if\n",
    "    # the flow for the corresponding features has been found, otherwise, it is set to 0.\n",
    "    good_new = nextPts[status==1]\n",
    "    good_prev = prevPts[status==1]\n",
    "    \n",
    "    # Use ravel to get points to draw lines and circles\n",
    "    for i,(new,prev) in enumerate(zip(good_new,good_prev)):\n",
    "        \n",
    "        x_new,y_new = new.ravel()\n",
    "        x_prev,y_prev = prev.ravel()\n",
    "        \n",
    "        # Lines will be drawn using the mask created from the first frame\n",
    "        mask = cv2.line(mask, (x_new,y_new),(x_prev,y_prev), (0,255,0), 3)\n",
    "        \n",
    "        # Draw red circles at corner points\n",
    "        frame = cv2.circle(frame,(x_new,y_new),8,(0,0,255),-1)\n",
    "    #----------------------------------\n",
    "    # Display the tracking results:\n",
    "    #----------------------------------\n",
    "    # - only do this for some frames\n",
    "    if ( frame_counter % 100 == 0 ):\n",
    "        # add the frame with overlays to the mask \n",
    "        img = cv2.add(frame,mask)\n",
    "        # display the image with overlays\n",
    "        cv2.imshow('Tracker: Lucas-Kanade Optical Flow on frame #: ' + str(frame_counter + 1),img)\n",
    "        # save the figure\n",
    "        # the first frame file name\n",
    "        output_file_path = \"../results/frame-\" + str(frame_counter) + \"-optical-flow-tracker.jpg\"\n",
    "        # save the frame\n",
    "        cv2.imwrite(output_file_path, img);\n",
    "    \n",
    "    # increment the frame counter\n",
    "    frame_counter = frame_counter + 1\n",
    "    \n",
    "    # quit if user hits: ESC\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "   \n",
    "    # Now update the previous frame and previous points\n",
    "    prev_gray = frame_gray.copy()\n",
    "    prevPts = good_new.reshape(-1,1,2)\n",
    "    \n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2) Tracker:  Dense Optical Flow\n",
    "\n",
    "* In this section, we shall implement the Dense Optical Flow tracker in OpenCV Python:\n",
    "  * Dense optical flow attempts to compute the optical flow vector for every pixel of each frame. \n",
    "  * While such computation may be slower, it gives a more accurate result and a denser result suitable \n",
    "    for applications such as learning structure from motion and video segmentation.\n",
    "  * Since Dense Optical Flow tracks every pixel, it does not require an intial set of points or region to track\n",
    "  * Dense optical flow highlights the pixels that are moving/changing faster than the rest of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the video file\n",
    "cap = cv2.VideoCapture(video_file_path)\n",
    "# check the status of the opened video file\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot read video file: \" + video_file_path)\n",
    "    exit();\n",
    "    \n",
    "# Grab first frame    \n",
    "ret, frame1 = cap.read()\n",
    "\n",
    "# Get gray scale image of first frame and make a mask in HSV color\n",
    "prvsImg = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "hsv_mask = np.zeros_like(frame1)\n",
    "hsv_mask[:,:,1] = 255\n",
    "\n",
    "# frame counter\n",
    "frame_counter = 1;\n",
    "\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "    nextImg = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Check out the markdown text above for a break down of these paramters, most of these are just suggested defaults\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvsImg,nextImg, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    \n",
    "    # Color the channels based on the angle of travel\n",
    "    # Pay close attention to your video, the path of the direction of flow will determine color!\n",
    "    mag, ang = cv2.cartToPolar(flow[:,:,0], flow[:,:,1],angleInDegrees=True)\n",
    "    hsv_mask[:,:,0] = ang/2\n",
    "    hsv_mask[:,:,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    \n",
    "    #----------------------------------\n",
    "    # Display the tracking results:\n",
    "    #----------------------------------\n",
    "    # - only do this for some frames\n",
    "    if ( frame_counter % 100 == 0 ):\n",
    "        # Convert back to BGR to show with imshow from cv\n",
    "        bgr = cv2.cvtColor(hsv_mask,cv2.COLOR_HSV2BGR)\n",
    "        # display the color image \n",
    "        cv2.imshow('Tracker: Dense Optical Flow on frame #: ' + str(frame_counter + 1),bgr)\n",
    "        # save the figure\n",
    "        # the first frame file name\n",
    "        output_file_path = \"../results/frame-\" + str(frame_counter) + \"-dense-optical-flow-tracker.jpg\"\n",
    "        # save the frame\n",
    "        cv2.imwrite(output_file_path, bgr);\n",
    "    \n",
    "    # increment the frame counter\n",
    "    frame_counter = frame_counter + 1\n",
    "    \n",
    "    # quit if user hits: ESC\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    \n",
    "    # Set the Previous image as the next iamge for the loop\n",
    "    prvsImg = nextImg\n",
    "\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3) Tracker:  Mean-Shift Tracker\n",
    "\n",
    "* In this section, we shall implement the Mean-Shift Tracker in OpenCV Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the video file\n",
    "cap = cv2.VideoCapture(video_file_path)\n",
    "# check the status of the opened video file\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot read video file: \" + video_file_path)\n",
    "    exit();\n",
    "    \n",
    "# take first frame of the video\n",
    "ret,frame = cap.read()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set Up the Initial Tracking Window:\n",
    "#------------------------------------------------------------\n",
    "# - This is set to the manually selected object of interest \n",
    "#   ROI/bounding-box previously selected\n",
    "#------------------------------------------------------------\n",
    "#--------------------------------------------\n",
    "# The object of interest manually annotated \n",
    "# bounding-box:\n",
    "#--------------------------------------------\n",
    "# TLC coordinates: (tlc_x, tlc_y)\n",
    "# tlc-x\n",
    "# tlc_x = 445\n",
    "# tlc-y\n",
    "# tlc_y = 100\n",
    "# the bounding-box dimension\n",
    "# width\n",
    "# width = 76\n",
    "# height\n",
    "# height = 183\n",
    "#--------------------------------------------\n",
    "# setup the tracked window bounding-box in \n",
    "# the form: (x,y,w,h)\n",
    "#--------------------------------------------\n",
    "track_window = (tlc_x, tlc_y, width, height)\n",
    "# set up the ROI for tracking\n",
    "roi = frame[tlc_y:tlc_y+height, tlc_x:tlc_x+width]\n",
    "\n",
    "# Use the HSV Color Mapping\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Find histogram to backproject the target on each frame for calculation of meanshit\n",
    "roi_hist = cv2.calcHist([hsv_roi],[0],None,[180],[0,180])\n",
    "\n",
    "# Normalize the histogram array values given a min of 0 and max of 255\n",
    "cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria, either 10 iteration or move by at least 1 pt\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "# frame counter\n",
    "frame_counter = 1;\n",
    "\n",
    "# iterate over the farmes\n",
    "while True:\n",
    "    ret ,frame = cap.read()\n",
    "    if ret == True:\n",
    "        \n",
    "        # Grab the Frame in HSV\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Calculate the Back Projection based off the roi_hist created earlier\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "        \n",
    "        # Apply meanshift to get the new coordinates of the rectangle\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "        \n",
    "        #----------------------------------\n",
    "        # Display the tracking results:\n",
    "        #----------------------------------\n",
    "        # - only do this for some frames\n",
    "        if ( frame_counter % 100 == 0 ):  \n",
    "            # Draw the new rectangle on the image\n",
    "            x,y,w,h = track_window\n",
    "            img2 = cv2.rectangle(frame, (x,y), (x+w,y+h), (0,0,255),5)\n",
    "            # display the image\n",
    "            cv2.imshow('Tracker: Mean-Shift on frame #: ' + str(frame_counter + 1),img2)\n",
    "            # save the figure\n",
    "            # the first frame file name\n",
    "            output_file_path = \"../results/frame-\" + str(frame_counter) + \"-mean-shift-tracker.jpg\"\n",
    "            # save the frame\n",
    "            cv2.imwrite(output_file_path, img2);\n",
    "\n",
    "        # increment the frame counter\n",
    "        frame_counter = frame_counter + 1\n",
    "        \n",
    "        # quit if user hits: ESC\n",
    "        k = cv2.waitKey(1) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4) Tracker:  Cam-Shift Tracker\n",
    "\n",
    "* In this section, we shall implement the Cam-Shift Tracker in OpenCV Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the video file\n",
    "cap = cv2.VideoCapture(video_file_path)\n",
    "# check the status of the opened video file\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot read video file: \" + video_file_path)\n",
    "    exit();\n",
    "# take first frame of the video\n",
    "ret,frame = cap.read()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Set Up the Initial Tracking Window:\n",
    "#------------------------------------------------------------\n",
    "# - This is set to the manually selected object of interest \n",
    "#   ROI/bounding-box previously selected\n",
    "#------------------------------------------------------------\n",
    "#--------------------------------------------\n",
    "# The object of interest manually annotated \n",
    "# bounding-box:\n",
    "#--------------------------------------------\n",
    "# TLC coordinates: (tlc_x, tlc_y)\n",
    "# tlc-x\n",
    "# tlc_x = 445\n",
    "# tlc-y\n",
    "# tlc_y = 100\n",
    "# the bounding-box dimension\n",
    "# width\n",
    "# width = 76\n",
    "# height\n",
    "# height = 183\n",
    "#--------------------------------------------\n",
    "# setup the tracked window bounding-box in \n",
    "# the form: (x,y,w,h)\n",
    "#--------------------------------------------\n",
    "track_window = (tlc_x, tlc_y, width, height)\n",
    "# set up the ROI for tracking\n",
    "roi = frame[tlc_y:tlc_y+height, tlc_x:tlc_x+width]\n",
    "\n",
    "# Use the HSV Color Mapping\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Find histogram to backproject the target on each frame for calculation of meanshit\n",
    "roi_hist = cv2.calcHist([hsv_roi],[0],None,[180],[0,180])\n",
    "\n",
    "# Normalize the histogram array values given a min of 0 and max of 255\n",
    "cv2.normalize(roi_hist,roi_hist,0,255,cv2.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria, either 10 iteration or move by at least 1 pt\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "# frame counter\n",
    "frame_counter = 1;\n",
    "\n",
    "# iterate over the farmes\n",
    "while True:\n",
    "    ret ,frame = cap.read()\n",
    "    if ret == True:\n",
    "        \n",
    "        # Grab the Frame in HSV\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Calculate the Back Projection based off the roi_hist created earlier\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "        \n",
    "        # Apply Camshift to get the new coordinates of the rectangle\n",
    "        ret, track_window = cv2.CamShift(dst, track_window, term_crit)\n",
    "       \n",
    "        #----------------------------------\n",
    "        # Display the tracking results:\n",
    "        #----------------------------------\n",
    "        # - only do this for some frames\n",
    "        if ( frame_counter % 100 == 0 ):  \n",
    "            # Draw the new rectangle on the image\n",
    "            pts = cv2.boxPoints(ret)\n",
    "            pts = np.int0(pts)\n",
    "            img2 = cv2.polylines(frame,[pts],True, (0,0,255),5)\n",
    "            cv2.imshow('img2',img2)\n",
    "            # display the image\n",
    "            cv2.imshow('Tracker: Cam-Shift on frame #: ' + str(frame_counter + 1),img2)\n",
    "            # save the figure\n",
    "            # the first frame file name\n",
    "            output_file_path = \"../results/frame-\" + str(frame_counter) + \"-cam-shift-tracker.jpg\"\n",
    "            # save the frame\n",
    "            cv2.imwrite(output_file_path, img2);\n",
    "        \n",
    "        # increment the frame counter\n",
    "        frame_counter = frame_counter + 1\n",
    "        \n",
    "        # quit if user hits: ESC\n",
    "        k = cv2.waitKey(1) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5) Additional Object Tracking API:\n",
    "\n",
    "* In this section, we shall implement the Tracking APIs (Built-in with OpenCV):\n",
    "    * We get the following options to experiment with the following trackers:\n",
    "      * Enter 0 for BOOSTING\n",
    "      * Enter 1 for MIL\n",
    "      * Enter 2 for KCF\n",
    "      * Enter 3 for TLD\n",
    "      * Enter 4 for MEDIANFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gets the user tracker selection:\n",
    "'''\n",
    "def ask_for_tracker():\n",
    "    print(\"Welcome! What Tracker API would you like to use?\")\n",
    "    print(\"Enter 0 for BOOSTING: \")\n",
    "    print(\"Enter 1 for MIL: \")\n",
    "    print(\"Enter 2 for KCF: \")\n",
    "    print(\"Enter 3 for TLD: \")\n",
    "    print(\"Enter 4 for MEDIANFLOW: \")\n",
    "    choice = input(\"Please select your tracker: \")\n",
    "    \n",
    "    if choice == '0':\n",
    "        tracker = cv2.TrackerBoosting_create()\n",
    "    if choice == '1':\n",
    "        tracker = cv2.TrackerMIL_create()\n",
    "    if choice == '2':\n",
    "        tracker = cv2.TrackerKCF_create()\n",
    "    if choice == '3':\n",
    "        tracker = cv2.TrackerTLD_create()\n",
    "    if choice == '4':\n",
    "        tracker = cv2.TrackerMedianFlow_create()\n",
    "\n",
    "\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! What Tracker API would you like to use?\n",
      "Enter 0 for BOOSTING: \n",
      "Enter 1 for MIL: \n",
      "Enter 2 for KCF: \n",
      "Enter 3 for TLD: \n",
      "Enter 4 for MEDIANFLOW: \n",
      "Please select your tracker: 4\n",
      "User selected Tracker: TrackerMedianFlow\n"
     ]
    }
   ],
   "source": [
    "# get the Tracker option from the user\n",
    "tracker = ask_for_tracker()\n",
    "# display the selected tracker\n",
    "print(\"User selected Tracker: \" + str(tracker).split()[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! What Tracker API would you like to use?\n",
      "Enter 0 for BOOSTING: \n",
      "Enter 1 for MIL: \n",
      "Enter 2 for KCF: \n",
      "Enter 3 for TLD: \n",
      "Enter 4 for MEDIANFLOW: \n",
      "Please select your tracker: 4\n"
     ]
    }
   ],
   "source": [
    "# Get the tracker option from the user\n",
    "tracker = ask_for_tracker()\n",
    "# get the tracker name\n",
    "tracker_name = str(tracker).split()[0][1:]\n",
    "\n",
    "# open the video file\n",
    "cap = cv2.VideoCapture(video_file_path)\n",
    "# check the status of the opened video file\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot read video file: \" + video_file_path)\n",
    "    exit();\n",
    "# take first frame of the video\n",
    "ret,frame = cap.read()\n",
    "\n",
    "# Special function allows us to draw on the very first frame our desired ROI\n",
    "roi = cv2.selectROI(frame, False)\n",
    "\n",
    "# Initialize tracker with first frame and bounding box\n",
    "ret = tracker.init(frame, roi)\n",
    "\n",
    "# frame counter\n",
    "frame_counter = 1;\n",
    "\n",
    "# iterate over the farmes\n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    \n",
    "    # Update tracker\n",
    "    success, roi = tracker.update(frame)\n",
    "    \n",
    "    # roi variable is a tuple of 4 floats\n",
    "    # We need each value and we need them as integers\n",
    "    (x,y,w,h) = tuple(map(int,roi))\n",
    "    \n",
    "    # Draw Rectangle as Tracker moves\n",
    "    if success:\n",
    "        # Tracking success\n",
    "        p1 = (x, y)\n",
    "        p2 = (x+w, y+h)\n",
    "        cv2.rectangle(frame, p1, p2, (0,255,0), 3)\n",
    "    else :\n",
    "        # Tracking failure\n",
    "        cv2.putText(frame, \"Failure to Detect Tracking!!\", (100,200), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,255),3)\n",
    "\n",
    "    # Display tracker type on frame\n",
    "    cv2.putText(frame, tracker_name, (20,400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),3);\n",
    "\n",
    "    #----------------------------------\n",
    "    # Display the tracking results:\n",
    "    #----------------------------------\n",
    "    # - only do this for some frames\n",
    "    if ( frame_counter % 100 == 0 ):  \n",
    "        # Draw the new rectangle on the image\n",
    "        cv2.imshow('Tracker: ' + tracker_name + 'on frame #: ' + str(frame_counter + 1), frame)\n",
    "        # save the figure\n",
    "        # the first frame file name\n",
    "        output_file_path = \"../results/frame-\" + str(frame_counter) + '--' + tracker_name + \".jpg\"\n",
    "        # save the frame\n",
    "        cv2.imwrite(output_file_path, frame);\n",
    "        \n",
    "    # increment the frame counter\n",
    "    frame_counter = frame_counter + 1\n",
    "        \n",
    "    # Exit if ESC pressed\n",
    "    k = cv2.waitKey(1) & 0xff\n",
    "    if k == 27 : \n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Display a successful execution message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program executed successfully on: 2021-04-07 20:10:11...Goodbye!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display a final message\n",
    "# current time\n",
    "now = datetime.datetime.now()\n",
    "# display a message\n",
    "print('Program executed successfully on: '+ str(now.strftime(\"%Y-%m-%d %H:%M:%S\") + \"...Goodbye!\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
